\documentclass[preview]{standalone}

\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{stellar}
\usepackage{definitions}

\begin{document}

\id{matrices-basic-definition}
\genpage

\section{Definition}

\includesnpt{matrix-definition}

\begin{snippetdefinition}{set-of-matrices-definition}{Set of matrices}
    Let \(R\) be a \commutativering. The set of \matrix[matrices]
    of size \(n\times m\) over \(R\) is denoted
    \[M_{n\times m}(R)\]
\end{snippetdefinition}

% the properties like commutativity and associativity (+)
% associative product
% are there only if the elements have it

\includesnpt{linearalgebra-zero-matrix-definition}

\includesnpt{linearalgebra-matrix-addition-definition}

\includesnpt{linearalgebra-matrix-scalar-multiplication-definition}

% TODO properties of addition
% A+0_{m,n} = A
% A+B = B + A
% 0A = 0_{m,n}
% A+(-A)= 0_{m,n}
% (A+B)+C = A + (B + C)
% 1A=A
% (a+b) A = aA + bA
% a(A+B) = aA+aB
% a(bA) = (ab)A

\includesnpt{linearalgebra-matrix-multiplication-definition}

\begin{snippet}{linearalgebra-expl3}
This means applying a `dot product' between every row and every column. \\

\[
    \begin{bmatrix} 
        a_1 && a_2 && a_3 && a_4 \\
        \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
        c_1 && c_2 && c_3 && c_4
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        d_1 && \mathbf{e_1} \\
        d_2 && \mathbf{e_2} \\
        d_3 && \mathbf{e_3} \\
        d_4 && \mathbf{e_4}
    \end{bmatrix}
    =
    \begin{bmatrix} 
        a \cdot d && a \cdot e \\
        b \cdot d && \mathbf{b \cdot e} \\
        c \cdot d && c \cdot e
    \end{bmatrix}
\]

Where \(a \cdot b\) denotes the `dot product' between a row and a column.

This operation is not commutative, but it is associative.
\end{snippet}

\includesnpt{kronecker-delta-definition}

\includesnpt{identity-matrix-definition}

% When \(I_n\) is applied to a matrix or vector, the matrix or vector remains the same.

\includesnpt{kronecker-delta-sifting-property}

\begin{snippet}{linearalgebra-expl4}
This is given by the fact that \(\delta_{i,k}\)
will be \(1\) only when \(i = k\).
\end{snippet}

% TODO properties fo mul and proofs
% A0_{n,p} = 0_{m,p}
% 0_{p,m}A=0_{p,n}

\includesnpt{linearalgebra-matrix-premultiplication}
\includesnpt{linearalgebra-matrix-postmultiplication}

\includesnpt{linearalgebra-matrix-identity-postmultiplication}
\includesnpt{linearalgebra-matrix-identity-postmultiplication-proof}
\includesnpt{linearalgebra-matrix-identity-premultiplication}
\includesnpt{linearalgebra-matrix-identity-premultiplication-proof}

\includesnpt{linearalgebra-polynomial-of-a-matrix-definition}

\includesnpt{linearalgebra-matrix-exponentiation-definition}
\includesnpt{linearalgebra-matrix-exponentiation-example-1}
\includesnpt{linearalgebra-matrix-exponentiation-example-2}

\includesnpt{linearalgebra-matrix-inverse-definition}
\includesnpt{linearalgebra-invertible-matrix-definition}

\begin{snippetproposition}{matrix-invertibility}{Matrix invertibility}
    If \(\det(A) = 0\), meaning that the matrix collapses space into a lower dimension,
    thus resulting in a loss of information, the matrix is not invertible.
    Otherwise, the matrix has an inverse and it is unique.
    
    This is equivalent to saying that if \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\),
    then \(A\) has no inverse.
\end{snippetproposition}

\includesnpt{linearalgebra-uniqueness-of-matrix-inverse}
\includesnpt{linearalgebra-uniqueness-of-matrix-inverse-proof}

\includesnpt{linearalgebra-product-rule-of-inverse-matrices}
\includesnpt{linearalgebra-product-rule-of-inverse-matrices-proof}

\includesnpt{linearalgebra-involution-rule-for-matrix-inverse}
\includesnpt{linearalgebra-involution-rule-for-matrix-inverse-proof}

\includesnpt{linearalgebra-matrix-left-and-write-inverses}
\includesnpt{linearalgebra-inverse-of-a-non-square-matrix}
\includesnpt{linearalgebra-inverse-of-a-square-matrix}

\begin{snippet}{linearalgebra-expl5}
If \(\det(M) = 0\), meaning that the matrix collapses space into a lower dimension,
thus resulting in a loss of information, the matrix is not invertible.

This is equivalent to saying that if \(M\vec{v}=0\) for some non-zero vector \(\vec{v}\),
then \(M\) has no inverse.
\end{snippet}

% TODO: commutability matrix theorem

\section{Matrix transpose}

\begin{snippetdefinition}{matrix-transpose-definition}{Matrix transpose}
    The \textit{transpose} of a \(n \times m\) \matrix results in a \(m \times n\) one.

    \[
        {\left(A_{ij}\right)}^t=A_{ji}
    \]
    
    The transpose of a matrix is just a flipped version of the original matrix.
    We can transpose a matrix by switching its rows with its columns.
    The original rows become the new columns and the original columns become the new rows.
    
    \[
        {\begin{bmatrix} 
            a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
            a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
        \end{bmatrix}}^t
        =
        \begin{bmatrix} 
            a_{1,1} & a_{2,1} & \cdots & a_{m,1} \\
            a_{1,2} & a_{2,2} & \cdots & a_{m,2} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{1,n} & a_{2,n} & \cdots & a_{m,n} 
        \end{bmatrix}
    \]
\end{snippetdefinition}

\section{Other definitions}

\includesnpt{diagonal-matrix-definition}
\includesnpt{upper-triangular-matrix-definition}
\includesnpt{lower-triangular-matrix-definition}

\textit{A diagonal matrix is equivalent to a matrix that is both upper triangular and lower triangular.}

\end{document}
