\documentclass[preview]{standalone}

\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{stellar}
\usepackage{definitions}

\begin{document}

\id{matrices-basic-definition}
\genpage

\section{Definition}

\includesnpt{matrix-definition}

\begin{snippetexample}{matrix-notation-example}{Matrix notation}
    A \matrix with size \(n \times m\) can be represented as
    \[
    A \equiv \begin{bmatrix} 
            a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
            a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
        \end{bmatrix}
    \]
    where \(a_{i,j} \triangleq A((i,j))\).
\end{snippetexample}

\includesnpt{linearalgebra-zero-matrix-definition}

\includesnpt{linearalgebra-matrix-addition-definition}

\includesnpt{linearalgebra-matrix-scalar-multiplication-definition}

\includesnpt{linearalgebra-matrix-multiplication-definition}

\begin{snippet}{linearalgebra-expl3}
    This means applying a `dot product' between every row and every column. \\

    \[
        \begin{bmatrix} 
            a_1 && a_2 && a_3 && a_4 \\
            \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
            c_1 && c_2 && c_3 && c_4
        \end{bmatrix}
        \cdot
        \begin{bmatrix} 
            d_1 && \mathbf{e_1} \\
            d_2 && \mathbf{e_2} \\
            d_3 && \mathbf{e_3} \\
            d_4 && \mathbf{e_4}
        \end{bmatrix}
        =
        \begin{bmatrix} 
            a \cdot d && a \cdot e \\
            b \cdot d && \mathbf{b \cdot e} \\
            c \cdot d && c \cdot e
        \end{bmatrix}
    \]

    Where \(a \cdot b\) denotes the `dot product' between a row and a column.

    This operation is not commutative, but it is associative.
\end{snippet}

\begin{snippetproposition}{matrix-multiplication-associative}{Associativity of matrix multiplication}
    Let \(A \in \matrices_{m,n}(\mathbb{F})\) e \(B \in \matrices_{n,p}(\mathbb{F})\) e \(C \in \matrices_{p,r} (\mathbb{F})\),
    then 
    \[
        (AB)C = A(BC)
    \]
\end{snippetproposition}

\begin{snippetproof}{matrix-multiplication-associative-proof}{matrix-multiplication-associative}{Associativity of matrix multiplication}
    \(AB\) has dimension \(m \times p\). The element at the position \((i,j)\) is given by
    
    \[
        \sum_{k=1}^n A_{i,j}B_{k,j} = D_{i,j}
    \]
    The \matrix \((AB)C\) has dimension \(m \times r\). The element at the position \((i,l)\) is
    \begin{align*}
        \sum_{j=1}^p D_{i,j}C_{j,l} &= \sum_{j=1}^p \left( \sum_{k=1}^n A_{i,k}B_{k,j} \right) C_{j,l} \\
        &= \sum_{j=1}^p \sum_{k=1}^n A_{i,k} B_{k,j} C_{j,l}
    \end{align*}
    \(BC\) has dimension \(n \times r\). The element at the position \((k,l)\) is
    \[
        \sum_{j=1}^p B_{k,j} C_{j,e} = E_{k,e}
    \]
    \(A(BC)\) has dimension \(m \times r\). The element at the position \((i,l)\) is
    \begin{align*}
        \sum_{k=1}^n A_{i,k}E_{k,l} &= \sum_{k=1}^n A_{i,k} \left( \sum_{j=1}^p B_{k,j}C_{j,e} \right)\\
        &= \sum_{k=1}^n \sum_{j=1}^p A_{i,k} B_{k,j} C_{j,e}
    \end{align*}
\end{snippetproof}

\includesnpt{kronecker-delta-definition}

\includesnpt{identity-matrix-definition}

% When \(I_n\) is applied to a matrix or vector, the matrix or vector remains the same.

\includesnpt{kronecker-delta-sifting-property}

\begin{snippet}{linearalgebra-expl4}
This is given by the fact that \(\kronecker_{i,k}\)
will be \(1\) only when \(i = k\).
\end{snippet}

% TODO properties fo mul and proofs
% A0_{n,p} = 0_{m,p}
% 0_{p,m}A=0_{p,n}

\includesnpt{linearalgebra-matrix-premultiplication}
\includesnpt{linearalgebra-matrix-postmultiplication}

\includesnpt{linearalgebra-matrix-identity-postmultiplication}
\includesnpt{linearalgebra-matrix-identity-postmultiplication-proof}
\includesnpt{linearalgebra-matrix-identity-premultiplication}
\includesnpt{linearalgebra-matrix-identity-premultiplication-proof}

\includesnpt{linearalgebra-polynomial-of-a-matrix-definition}

\includesnpt{linearalgebra-matrix-exponentiation-definition}
\includesnpt{linearalgebra-matrix-exponentiation-example-1}
\includesnpt{linearalgebra-matrix-exponentiation-example-2}

\includesnpt{linearalgebra-matrix-inverse-definition}
\includesnpt{linearalgebra-invertible-matrix-definition}

\begin{snippetproposition}{matrix-invertibility}{Matrix invertibility}
    If \(\det(A) = 0\), meaning that the matrix collapses space into a lower dimension,
    thus resulting in a loss of information, the matrix is not invertible.
    Otherwise, the matrix has an inverse and it is unique.
    
    This is equivalent to saying that if \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\),
    then \(A\) has no inverse.
\end{snippetproposition}

\includesnpt{linearalgebra-uniqueness-of-matrix-inverse}
\includesnpt{linearalgebra-uniqueness-of-matrix-inverse-proof}

\includesnpt{linearalgebra-product-rule-of-inverse-matrices}
\includesnpt{linearalgebra-product-rule-of-inverse-matrices-proof}

\includesnpt{linearalgebra-involution-rule-for-matrix-inverse}
\includesnpt{linearalgebra-involution-rule-for-matrix-inverse-proof}

\includesnpt{linearalgebra-matrix-left-and-write-inverses}
\includesnpt{linearalgebra-inverse-of-a-non-square-matrix}
\includesnpt{linearalgebra-inverse-of-a-square-matrix}

\begin{snippet}{linearalgebra-expl5}
If \(\det(M) = 0\), meaning that the matrix collapses space into a lower dimension,
thus resulting in a loss of information, the matrix is not invertible.

This is equivalent to saying that if \(M\vec{v}=0\) for some non-zero vector \(\vec{v}\),
then \(M\) has no inverse.
\end{snippet}

% TODO: commutability matrix theorem

\section{Matrix transpose}

\begin{snippetdefinition}{matrix-transpose-definition}{Matrix transpose}
    The \textit{transpose} of a \(n \times m\) \matrix results in a \(m \times n\) one.

    \[
        {\left(A_{ij}\right)}^t=A_{ji}
    \]
    
    The transpose of a matrix is just a flipped version of the original matrix.
    We can transpose a matrix by switching its rows with its columns.
    The original rows become the new columns and the original columns become the new rows.
    
    \[
        {\begin{bmatrix} 
            a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
            a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
        \end{bmatrix}}^t
        =
        \begin{bmatrix} 
            a_{1,1} & a_{2,1} & \cdots & a_{m,1} \\
            a_{1,2} & a_{2,2} & \cdots & a_{m,2} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{1,n} & a_{2,n} & \cdots & a_{m,n} 
        \end{bmatrix}
    \]
\end{snippetdefinition}

\section{Other definitions}

\includesnpt{diagonal-matrix-definition}
\includesnpt{upper-triangular-matrix-definition}
\includesnpt{lower-triangular-matrix-definition}

\textit{A diagonal matrix is equivalent to a matrix that is both upper triangular and lower triangular.}

\end{document}
