\documentclass[preview]{standalone}

\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{stellar}
\usepackage{definitions}

\begin{document}

\id{matrices-basic-definition}
\genpage

\section{Definition}

\includesnpt{matrix-definition}

% the set of all m times n matrices?

% the properties like commutativity and associativity
% are there only if the elements have it

\section{Definitions}

\includesnpt{linearalgebra-zero-matrix-definition}

\includesnpt{linearalgebra-matrix-addition-definition}

\includesnpt{linearalgebra-matrix-scalar-multiplication-definition}

% TODO properties of addition
% A+0_{m,n} = A
% A+B = B + A
% 0A = 0_{m,n}
% A+(-A)= 0_{m,n}
% (A+B)+C = A + (B + C)
% 1A=A
% (a+b) A = aA + bA
% a(A+B) = aA+aB
% a(bA) = (ab)A

\includesnpt{linearalgebra-matrix-multiplication-definition}

\begin{snippet}{linearalgebra-expl3}
This means applying a `dot product' between every row and every column. \\

\[
    \begin{bmatrix} 
        a_1 && a_2 && a_3 && a_4 \\
        \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
        c_1 && c_2 && c_3 && c_4
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        d_1 && \mathbf{e_1} \\
        d_2 && \mathbf{e_2} \\
        d_3 && \mathbf{e_3} \\
        d_4 && \mathbf{e_4}
    \end{bmatrix}
    =
    \begin{bmatrix} 
        a \cdot d && a \cdot e \\
        b \cdot d && \mathbf{b \cdot e} \\
        c \cdot d && c \cdot e
    \end{bmatrix}
\]

Where \(a \cdot b\) denotes the `dot product' between a row and a column.

This operation is not commutative, but it is associative.
\end{snippet}

\includesnpt{linearalgebra-kronecker-delta-definition}

\includesnpt{linearalgebra-identity-matrix-definition}

% When \(I_n\) is applied to a matrix or vector, the matrix or vector remains the same.

\includesnpt{linearalgebra-kronecker-delta-sifting-property}

\begin{snippet}{linearalgebra-expl4}
This is given by the fact that \(\delta_{i,k}\)
will be \(1\) only when \(i = k\).
\end{snippet}

% TODO properties fo mul and proofs
% A0_{n,p} = 0_{m,p}
% 0_{p,m}A=0_{p,n}

\includesnpt{linearalgebra-matrix-premultiplication}
\includesnpt{linearalgebra-matrix-postmultiplication}

\includesnpt{linearalgebra-matrix-identity-postmultiplication}
\includesnpt{linearalgebra-matrix-identity-postmultiplication-proof}
\includesnpt{linearalgebra-matrix-identity-premultiplication}
\includesnpt{linearalgebra-matrix-identity-premultiplication-proof}

\includesnpt{linearalgebra-polynomial-of-a-matrix-definition}

\includesnpt{linearalgebra-matrix-exponentiation-definition}
\includesnpt{linearalgebra-matrix-exponentiation-example-1}
\includesnpt{linearalgebra-matrix-exponentiation-example-2}

\includesnpt{linearalgebra-matrix-inverse-definition}
\includesnpt{linearalgebra-invertible-matrix-definition}

\begin{snippetproposition}{matrix-invertibility}{Matrix invertibility}
    If \(\det(A) = 0\), meaning that the matrix collapses space into a lower dimension,
    thus resulting in a loss of information, the matrix is not invertible.
    Otherwise, the matrix has an inverse and it is unique.
    
    This is equivalent to saying that if \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\),
    then \(A\) has no inverse.
\end{snippetproposition}

\includesnpt{linearalgebra-uniqueness-of-matrix-inverse}
\includesnpt{linearalgebra-uniqueness-of-matrix-inverse-proof}

\includesnpt{linearalgebra-product-rule-of-inverse-matrices}
\includesnpt{linearalgebra-product-rule-of-inverse-matrices-proof}

\includesnpt{linearalgebra-involution-rule-for-matrix-inverse}
\includesnpt{linearalgebra-involution-rule-for-matrix-inverse-proof}

\includesnpt{linearalgebra-matrix-left-and-write-inverses}
\includesnpt{linearalgebra-inverse-of-a-non-square-matrix}
\includesnpt{linearalgebra-inverse-of-a-square-matrix}

\begin{snippet}{linearalgebra-expl5}
If \(\det(M) = 0\), meaning that the matrix collapses space into a lower dimension,
thus resulting in a loss of information, the matrix is not invertible.

This is equivalent to saying that if \(M\vec{v}=0\) for some non-zero vector \(\vec{v}\),
then \(M\) has no inverse.
\end{snippet}

% TODO: commutability matrix theorem

\section{Matrix transpose}

\begin{snippetdefinition}{matrix-transpose-definition}{Matrix transpose}
    The \textit{transpose} of a \(n \times m\) matrix results in a \(m \times n\) one.

    \[
        {\left(A_{ij}\right)}^t=A_{ji}
    \]
    
    The transpose of a matrix is just a flipped version of the original matrix.
    We can transpose a matrix by switching its rows with its columns.
    The original rows become the new columns and the original columns become the new rows.
    
    \[
        {\begin{bmatrix} 
            a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
            a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
        \end{bmatrix}}^t
        =
        \begin{bmatrix} 
            a_{1,1} & a_{2,1} & \cdots & a_{m,1} \\
            a_{1,2} & a_{2,2} & \cdots & a_{m,2} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{1,n} & a_{2,n} & \cdots & a_{m,n} 
        \end{bmatrix}
    \]
\end{snippetdefinition}

\section{Column space}

\begin{snippetdefinition}{column-space-definition}{Column space}
    The \textit{column space} (or range or image) of a matrix is the
    set of all possible vectors that can be generated using the transformation.
\end{snippetdefinition}

\section{Rank of a matrix}

\begin{snippetdefinition}{matrix-rank-definition}{Matrix rank}
    The \textit{rank} of a matrix \(M\) is the dimension of its column space.
\end{snippetdefinition}

\begin{snippet}{matrix-rank-expl}
    A rank \(n\) means that every vector after the transformation of \(M\)
    is projected in the \(n\)-dimension.
\end{snippet}

\begin{snippetdefinition}{matrix-full-rank-definition}{Matrix full rank}
    A matrix is \textit{full rank} if its rank is equal to the number of its columns, meaning
    the dimension does not change.
\end{snippetdefinition}

\begin{snippet}{full-rank-expl}
    A matrix is full rank iff \(\vec{v}=\vec{0}\) is the only vector such that \(M\vec{v}=\vec{0}\).
    
    Note that non-square matrices may still be full rank even if the vectors end up in a lower dimension.
    If there is no loss of information the matrix is still full rank.
    If \(M\) has dimensions \(m \times n\), then it is full rank if \(\text{Rank}(M)=\min(n,m)\).
\end{snippet}

\section{Null space}

\begin{snippetdefinition}{null-space-definition}{Null space}
    The \textit{null space} or the \textit{kernel} of a matrix is the set of all the vectors that land on the origin
    after the transformation.
    The kernel is the linear subspace of the domain of the transformation which is mapped to \(\vec{0}\).
\end{snippetdefinition}

\end{document}
