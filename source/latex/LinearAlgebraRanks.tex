\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stellar}
\usepackage{definitions}

\begin{document}

\id{linear-algebra-ranks}
\genpage

\section{Ranks}

\begin{snippetdefinition}{column-space-definition}{Column space}
    The \textit{column space} (or \emph{range} or \emph{image}) of a \matrix \(A\) is the
    image of the \lineartransformation defined as left-multiplication by \(A\).
\end{snippetdefinition}

\begin{snippetdefinition}{matrix-rank-definition}{Matrix rank}
    The \textit{rank} of a matrix \(M\) is the \lineardimtext of its column space.
\end{snippetdefinition}

\begin{snippetdefinition}{linear-transformation-rank-definition}{Linear transformation rank}
    Let \(f \in \setoflineartransformations\). Then, the \emph{rank} of
    \(f\) is defined as
    \[
        \text{rank} f \triangleq \lineardim \image f
    \]
\end{snippetdefinition}

\begin{snippet}{matrix-rank-expl}
    A rank of \(n\) means that every vector after the transformation of \(M\)
    is projected in the \(n\)-dimension.
\end{snippet}

\begin{snippetdefinition}{matrix-full-rank-definition}{Matrix full rank}
    A matrix is \textit{full rank} if its rank is equal to the number of its columns, meaning
    the dimension does not change.
\end{snippetdefinition}

\begin{snippet}{full-rank-expl}
    A matrix is full rank if \(\vec{v}=\vec{0}\) is the only vector such that \(M\vec{v}=\vec{0}\).
    
    Note that non-square matrices may still be full rank even if the vectors end up in a lower dimension.
    If there is no loss of information the matrix is still full rank.
    If \(M\) has dimensions \(m \times n\), then it is full rank if \(\text{Rank}(M)=\min(n,m)\).
\end{snippet}

\plain{The rank of a matrix and the rank of its associated linear transformation are actually the same.}

\begin{snippetproposition}{linear-transformation-matrix-rank-coincide}{}
    Let \(A \in \matrices_{m \times n}(\mathbb{F})\) where \(\mathbb{F}\) is a \field
    and let \(L_A \colon \mathbb{F}^n \fromto \mathbb{F}^n\) be a \lineartransformation defined as
    \(L_A(x) = Ax\). Then,
    \[
        \mrank A = \ltrank f
    \]
\end{snippetproposition}

\begin{snippetproof}{linear-transformation-matrix-rank-coincide-proof}{linear-transformation-matrix-rank-coincide}{}
    Applying \(L_a(e_i) = A^i\) on the \(i\)-th vector of the canonical \basis gives the \(i\)-th column of the matrix.
    Since \(\{e_1, \cdots, e_n\}\) is a \basis of \(\mathbb{F}^n\)
    the vectors \(\{L_A(e_1), \cdots, L_A(e_n)\}\) form a generating set of \(\image L_A\).
    But then, the rank of \(A\)
    \[
        \mrank A = \lineardim \linearspan (A^1, \cdots, A^n)
        = \lineardim \linearspan (L_A(e_1), \cdots, L_A(e_n)) = \ltrank L_A
    \]
\end{snippetproof}

\begin{snippettheorem}{rank-nullity-theorem}{Rank-nullity theorem}
    Let \(V, W\) be finite dimensional \vectorspace[vector spaces]
    and let \(f \in \setoflineartransformations(V, W)\). Then,
    \[
        \lineardim V = \lineardim \grpker f + \ltrank f
    \]
\end{snippettheorem}

\begin{snippetproof}{rank-nullity-theorem-proof}{rank-nullity-theorem}{Rank-nullity theorem}
    Since \(\grpker f\) is a linear subspace of \(V\), it is also of finite dimension and let
    \(m = \lineardim \grpker f\). Let \(\{v_1, v_2, \cdots, v_m\}\)
    be a \basis for \(\grpker f\). We can expand such a \set into a \basis for \(V\), namely
    \(\{v_1, v_2, \cdots, v_m, u_1, u_2, \cdots, u_n\}\) meaning that \(\lineardim V = n+m\).
    We now need to show that \(\lineardim \image f = n\).
    Let \(w\in \image f\), meaning \(w = f(v)\) for some \(v\in V\).
    Thus, there exist \(\alpha_i, \beta_j \in \mathbb{F}\) such that
    \begin{align*}
        v &= \sum_{i=1}^m \alpha_i v_i + \sum_{j=0}^n \beta_i u_n  \\
        w = f(v) &= \sum_{i=1}^m \alpha_i f(v_i) + \sum_{j=0}^n \beta_i f(u_n) \\
        &= \sum_{j=0}^n \beta_i f(u_n) \in \linearspan \{f(u_1), f(u_2), \cdots, f(u_n)\}
    \end{align*}
    In particular, \(\{f(u_1), f(u_2), \cdots, f(u_n)\}\) is a generating set of \(\image f\)
    (which is thus finite dimensional). We now need to show that \(\{f(u_1), f(u_2), \cdots, f(u_n)\}\)
    is \linearlyindependent. Consider some \(r_i \in \mathbb{F}\) such that
    \begin{align*}
        0_W &= \sum_{i=0}^n r_i f(u_i) \\
        &= f \left( \sum_{i=0}^n r_i u_i \right) \\
        &\implies \sum_{i=0}^n r_i u_i \in \grpker f
    \end{align*}
    But since \(\{v_1, \cdots, v_m, u_1, \cdots, u_n\}\) is a \basis of \(V\),
    and the kernel is generated by \(\{v_1, \cdots, v_m\}\),
    any element of the kernel must be a \linearcombination of the \(v_i\)'s.
    Therefore,
    \[
        \sum_{j=1}^n r_j u_j = \sum_{i=1}^m s_i v_i
    \]
    for some \(s_i \in \mathbb{F}\). Rearranging gives
    \[
        \sum_{i=1}^m (-s_i) v_i + \sum_{j=1}^n r_j u_j = 0
    \]
    which is a linear combination of basis vectors equal to zero. By \linearlyindependent[linear independence],
    all coefficients must vanish, so in particular \(r_j = 0\) for all \(j = 1, \ldots, n\).
    Hence \(\{f(u_1), \cdots, f(u_n)\}\) is \linearlyindependent.

    Therefore, \(\{f(u_1), \cdots, f(u_n)\}\) is a \basis of \(\image f\),
    so \(\lineardim \image f = n\). Since \(\lineardim V = n + m\) and \(m = \lineardim \grpker f\),
    it follows that
    \[
        \lineardim V = \lineardim \grpker f + \lineardim \image f
    \]
    as desired.
\end{snippetproof}

\end{document}