\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stellar}
\usepackage{definitions}
\usepackage{bettelini}

\begin{document}

\id{matrices-systems-of-linear-equations}
\genpage

\section{Elementary row operations}

\includesnpt{linearalgebra-elementary-row-operations}

\plain{We can construct matrices such when multiplied an elementary row addition operation is performed.}

\begin{snippetdefinition}{elementary-lambda-matrix-definition}{Elementary \(\lambda\) matrix}
    The \emph{elementary matrix} \(E_{i,j}(\lambda)\) is the \(n\times n\) \matrix defined as
    \[
        E_{i,j}(\lambda) = \identmatrix{n} + \lambda e_{i,j}
    \]
    where \(e_{i,k}\) is the \matrix with all entries zero expect for a \(1\) at position \((i,j)\).
\end{snippetdefinition}

\begin{snippetproposition}{elementary-matrix-row-operation}{}
    Let \(A\) be an \(n\times m\) \matrix. Then, the product \(E_{i,j}(\lambda)A\) is the \matrix obtained by
    adding \(\lambda\) times the \(j\)-th row of \(A\) to the \(i\)-th row, which is the
    \snippetref[linearalgebra-elementary-row-operations][elementary row addition operation].
\end{snippetproposition}

\begin{snippetproof}{elementary-matrix-row-operation-proof}{elementary-matrix-row-operation}{}
    Let \(A\) have rows rows \(a_1, a_2, \dots, a_n\), written as:
    \[
        A =
        \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
        \end{bmatrix}.
    \]
    Ccompute the matrix product \(E_{i,j}(\lambda) A\).
    Since \(E_{i,j}(\lambda) = I_n + \lambda e_{i,j}\), we expand:
    \[
        E_{i,j}(\lambda) A = (I_n + \lambda e_{i,j}) A = I_n A + \lambda e_{i,j} A.
    \]
    Since \(I_n A = A\), it remains to compute \(e_{i,j} A\).
    By the definition of \(e_{i,j}\), this matrix has only one nonzero row: it picks row \(j\) of \(A\)
    and places it in row \(i\), with all other rows being zero. Thus, we get:
    \[
        e_{i,j} A =
        \begin{bmatrix}
        0 \\ \vdots \\ 0 \\ a_j \\ 0 \\ \vdots \\ 0
        \end{bmatrix},
    \]
    where \(a_j\) appears in row \(i\), and all other rows are zero.
    Multiplying by \(\lambda\) gives:
    \[
        \lambda e_{i,j} A =
        \begin{bmatrix}
        0 \\ \vdots \\ 0 \\ \lambda a_j \\ 0 \\ \vdots \\ 0
        \end{bmatrix}.
    \]
    Adding this to \(A\), we see that only row \(i\) is modified:
    \[
        E_{i,j}(\lambda) A =
        \begin{bmatrix}
        a_1 \\ \vdots \\ a_{i-1} \\ a_i + \lambda a_j \\ a_{i+1} \\ \vdots \\ a_n
        \end{bmatrix}.
    \]
    Thus, row \(i\) is updated as \(a_i + \lambda a_j\), while all other rows remain unchanged.
\end{snippetproof}

\plain{We can also create a matrix to perform an elementary row swap}

\begin{snippetdefinition}{eleemntary-permutation-matrix-definition}{Elementary-permutation-matrix}
    The \emph{elementary permutation matrix} \(P_{i,j}\) is defined as the
    identity \(I_n\) but with rows \(i\) and \(j\) swapped.
\end{snippetdefinition}

\section{Systems of linear equations}

\begin{snippetproposition}{trivial-system-of-linear-equations-solution}{Trivial system of linear equations}
    Let \(a,b \in \realnumbers\). Then, the solution to
    \[
        ax = b
    \]
    is given by
    \[
        \begin{cases}
            x = \frac{b}{a} & a \neq 0 \\
            x \in \emptyset & a = 0 \land b \neq 0 \\
            x \in \realnumbers & a = 0 \land b = 0
        \end{cases}
    \]
\end{snippetproposition}

\begin{snippet}{systems-of-linear-equations}
    A system of linear equations
    \begin{align*}
        a_{1, 1}x + a_{1, 2}y + \cdots + a_{1, m}z &= b_1 \\
        a_{2, 1}x + a_{2, 2}y + \cdots + a_{2, m}z &= b_2 \\
        \cdots \\
        a_{n, 1}x + a_{n, 2}y + \cdots + a_{n, m}z &= b_n
    \end{align*}

    Can be represented by a matrix multiplication \(M\vec{x}=\vec{d}\)

    \[
        \begin{bmatrix} 
            a_{1, 1} && a_{1, 2} && \cdots && a_{1, m} \\
            a_{2, 1} && a_{2, 2} && \cdots && a_{2, m} \\
            \vdots && \vdots && \ddots && \vdots \\
            a_{n, 1} && b_{n, 2} && \cdots && a_{n, m}
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\ y_2 \\ \vdots \\ y_n
        \end{bmatrix}
        =
        \begin{bmatrix}
            b_1 \\ b_2 \\ \vdots \\ b_n
        \end{bmatrix}
    \]

    The geometrical interpretation is to find the vector \(\vec{x}\)
    such that when the matrix \(M\) is applied to it, the resulting vector is \(\vec{d}\).
    We can also represent it as finding the intersections of straight lines, or finding the coefficients
    for the linear combinations of the vectors to find the vector of the known-values.

    We may represent the whole system just by
    \[
        \begin{bmatrix} 
            a_{1, 1} && a_{1, 2} && \cdots && a_{1, m} && b_1 \\
            a_{2, 1} && a_{2, 2} && \cdots && a_{2, m} && b_2 \\
            \vdots && \vdots && \vdots && \ddots && \vdots \\
            a_{n, 1} && b_{n, 2} && \cdots && a_{n, m} && b_n
        \end{bmatrix}
    \]
\end{snippet}

\subsection{Using elementary row operations}

\begin{snippet}{linearalgebra-expl1}
    Applying \snippetref[linearalgebra-elementary-row-operations][elementary row operations]
    does not change the solution of the linear system.
\end{snippet}

\includesnpt{linearalgebra-lin-sys-sol-amount}

\begin{snippet}{linearalgebra-expl2}
    By applying these operations, our goal is to make the system matrix
    look like the following:
    \[
        \begin{bmatrix} 
            1 && 0 && 0 && e_1 \\
            0 && 1 && 0 && e_2 \\
            0 && 0 && 1 && e_3
        \end{bmatrix}
    \]
    which is the implicit solution
    \(x=e_1\), \(y=e_2\) and \(z=e_3\).
    If this is possible, then this is the only solution to our system.

    \vspace{.25cm}

    If it is possible to create a row whose elements are all \(0\)s,
    then there are infinitely many solutions, because
    there are infinitely many solutions for the equation
    \(0x_1+0x_2+\cdots+0x_n = 0\).

    \vspace{.25cm}

    If it is possible to create a whose elements elements
    are all \(0\)s expect for the last one there are zero solutions,
    because there are zero solutions for the equation
    \(0x_1+0x_2+\cdots+0x_n = a\) with \(a \neq 0\).
\end{snippet}

\begin{snippettheorem}{lu-decomposition-theorem}{LU Decomposition}
    Given a system of linear equations represented by \(Ax = b\).
    Given a decomposition of the form \(A = LU\) where \(L\) is a lower triangular
    matrix and \(U\) is an upper triangular matrix, we have the equation
    \[
        LUx = b
    \]
    Since \(Ux\) is a vector, let \(y = Ux\). By solving for \(y\) and then for \(x\),
    we get the solution to the system.
\end{snippettheorem}

\plain{The lower triangular matrix encodes the elementary operation, and the upper triangular
matrix is the system matrix in row echelon form.}

\begin{snippetdefinition}{nonsingular-matrix-definition}{Nonsingular matrix}
    A square \matrix is said to be \emph{nonsingular} if
    the associated system of equation \(Ax = b\) has only one solution for every \(b\).
\end{snippetdefinition}

\begin{snippettheorem}{nonsingular-matrix-invertible-theorem}{Nonsingular matrix invertibility}
    A square \matrix is invertible \ifandonlyif it is nonsingular.
\end{snippettheorem}

\subsubsection{Examples}

\includesnpt{linearalgebra-matrix-linear-system-1-sol-example-1}
\includesnpt{linearalgebra-matrix-linear-system-inf-sol-example-1}
\includesnpt{linearalgebra-matrix-linear-system-0-sol-example-1}

\section{Cramer's rule}

\begin{snippettheorem}{cramer-rule-theorem}{Cramer's rule}
    Consider a system of \(n\) equations and unknowns
    \[
        A\vec{x}=\vec{b}
    \]
    The solution is given by
    \[
        \vec{x}_i = \frac{\det(A_i)}{\det(A)}
    \]
    where \(A_i\) is formed by replacing the \(i\)-th column
    of \(A\) by \(\vec{b}\).
\end{snippettheorem}

\section{Unique solution}

\begin{snippetproposition}{two-by-two-syste-unique-solution}{}
    Consider the system of equations
    \[
        \begin{cases}
            ax + by = c \\
            \alpha x + \beta y = \gamma
        \end{cases}
    \]
    This system has a unique solution \ifandonlyif \(\Delta = a\beta - b\alpha \neq 0\)
    given by
    \[
        \begin{cases}
            x = \frac{c\beta - b\gamma}{\Delta} \\
            y = \frac{\alpha\gamma - c\alpha}{\Delta}
        \end{cases}
    \]
\end{snippetproposition}

\begin{snippetproof}{two-by-two-syste-unique-solution-proof}{two-by-two-syste-unique-solution}{}
    \iffproof{
        By substituting \(x\) and \(y\) we can see that it is a solution.
    }{
        If there were multiple solutions \((x_0, y_0) \neq (x_1, y_1)\)
        we would have
        \[
            (x_0 - x_1) \begin{pmatrix} a \\ \alpha \end{pmatrix}
             + (y_0 - y_1) \begin{pmatrix} b \\ \beta \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
        \]
        but this happens \ifandonlyif \(\Delta = 0\), and thus it is absurd \lightning.
    }
\end{snippetproof}

\end{document}