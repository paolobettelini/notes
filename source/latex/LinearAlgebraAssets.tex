\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{stellar}
\usepackage{definitions}
\usepackage{bettelini}

\begin{document}

\begin{snippetdefinition}{vector-space-definition}{Vector space}
    A \textit{vector space} (or \textit{linear space}) over a \field \((\mathbb{F}, +, \cdot)\) is a quadruple
    \((V, \mathbb{F}, +, \cdot)\) where
    \begin{itemize}
        \item \(V\) is a non-empty \set whose elements are called \textit{vectors};
        \item \((\mathbb{F}, +, \cdot)\) is a \field whose elements are called \textit{scalars};
        \item \(+\) is a \binoperation \(+\colon V \cartesianprod V \fromto V\) called \textit{vector addition};
        \item \(\cdot\) is a \binoperation \(\cdot\colon \mathbb{F} \cartesianprod V \fromto V\) called \textit{scalar multiplication};
    \end{itemize}
    such that for all \(\vec{u}, \vec{v}, \vec{w} \in V\) and \(a,b \in \mathbb{F}\)
    \begin{enumerate}
        \item \((V, +)\) is an \abeliangroup;
        \item \emph{distributivity of scalar multiplication with respect to vector addition}: 
        \(a \cdot (\vec{u} + \vec{v}) = a \cdot \vec{u} + a \cdot \vec{v}\);
        \item \emph{distributivity of scalar multiplication with respect to field addition}: 
        \((a + b) \cdot \vec{v} = a \cdot \vec{v} + b \cdot \vec{v}\);
        \item \emph{compatibility of scalar multiplication with field multiplication}: 
        \((a \cdot b) \cdot \vec{v} = a \cdot (b \cdot \vec{v})\);
        \item \emph{identity element of scalar multiplication}: 
        \(1 \cdot \vec{v} = \vec{v}\)
        where \(1\) is the multiplicative identity in \(\mathbb{F}\).
    \end{enumerate}
\end{snippetdefinition}

\begin{snippetdefinition}{linear-combination-definition}{Linear Combination}
    Let \(V\) be a \vectorspace,
    \(\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n}\) be \vector[vectors] in \(V\) and
    \(a_1, a_2, \cdots, a_n\) be \vsscalar[scalars] in \(V\).
    A \textit{linear combination} is the sum
    \[
        \sum_{k=1}^n a_k\vec{v_k}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{span-definition}{Span}
    Let \(V\) be a \vectorspace over a \field \(K\) and \(S\) a \set of \vector[vectors] in \(V\).
    The \textit{span} of \(S\) is the set of all finite \linearcombination[linear combinations]
    of \(S\)
    \[
        \text{span}(S) = \left\{
            \sum_{n=1}^k \lambda_n\vec{v_n} \ \middle|\
            k \in \naturalnumbers, \vec{v_n} \in S, \lambda_n \in K
        \right\}
    \]
    and \(\text{span}(\emptyset) = \{0_V\}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linear-independence-definition}{Linear independence}
    Let \(V\) be a \vectorspace over a \field \(K\) and \(S\) a \set of \vector[vectors] in \(V\).
    The \vector[vectors] \(S\) are \textit{linearly independent} if the only solution to
    \[ \sum_{i=1}^n a_i v_i = 0\]
    is \(a_i = 0\)
    where \(a_1, a_2, \cdots, a_n\) are \vsscalar[scalars] in \(K\) and \(v_i \in V\).
\end{snippetdefinition}

\begin{snippetdefinition}{matrix-definition}{Matrix}
    Let \(\mathbb{F}\) be a \field and \(m,n \in {\naturalnumbers}^\exceptzero\).
    A \emph{matrix} over a \(\mathbb{F}\) of \emph{dimension} \(n \times m\)
    if a \function
    \[
        A \colon \{1, 2, \ldots, n\} \cartesianprod \{1, 2, \ldots, m\} \fromto \mathbb{F}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{diagonal-matrix-definition}{Diagonal matrix}
    Let \(M\) be a square \matrix with entries \(a_{i,j}\).
    Then, \(M\) is \textit{diagonal} if \(i \neq j \implies a_{i,j} = 0\). 
\end{snippetdefinition}

\begin{snippetdefinition}{upper-triangular-matrix-definition}{Upper triangular matrix}
    Let \(M\) be a square \matrix with entries \(a_{i,j}\).
    Then, \(M\) is \textit{upper triangular} if \(i > j \implies a_{i,j} = 0\). 
\end{snippetdefinition}

\begin{snippetdefinition}{lower-triangular-matrix-definition}{Lower triangular matrix}
    Let \(M\) be a square \matrix with entries \(a_{i,j}\).
    Then, \(M\) is \textit{upper triangular} if \(i < j \implies a_{i,j} = 0\). 
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-elementary-row-operations}{Elementary row operations}
    \begin{itemize}
        \item \textbf{Elementary row switching}:
        To switch two rows of a matrix.
        \item \textbf{Elementary row multiplication}:
        To multiply each element of a row by
        a scalar \(k \neq 0\).
        \item \textbf{Elementary row addition}:
        To add a multiple of every element of a row
        to each respective element of a another row.
    \end{itemize}

\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-lin-sys-sol-amount}{Solutions of a system of linear equations}
    A system of linear equations can have \(0\), \(1\)
    or an infinite amount of solutions.
\end{snippetproposition}

\begin{snippetexample}{linearalgebra-matrix-linear-system-1-sol-example-1}{Matrix linear system 1 solution}
    \[
        \begin{bmatrix}
            1 & 1 & 1 & 2 \\
            3 & 1 & -2 & -2 \\
            2 & 4 & 1 & 0
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & 1 \\
            0 & 1 & 0 & -1 \\
            0 & 0 & 1 & 2
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-linear-system-inf-sol-example-1}{Matrix linear system infinite solutions}
    \[
        \begin{bmatrix}
            2 & -1 & 1 & 3 & 2 \\
            2 & -1 & 1 & 2 & 4 \\
            4 & -3 & 3 & 8 & 8
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & -1 & 2 \\
            0 & 1 & -1 & -4 & 0 \\
            0 & 0 & 0 & 0 & 0
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-linear-system-0-sol-example-1}{Matrix linear system 0 solutions}
    \[
        \begin{bmatrix}
            1 & 1 & 1 & 1 & 4 \\
            2 & 3 & -2 & -3 & 1 \\
            1 & 0 & 5 & 6 & 1
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & -5 & -6 & -11 \\
            0 & 1 & -4 & -9 & -7 \\
            0 & 0 & 0 & 0 & 1
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetdefinition}{linearalgebra-zero-matrix-definition}{Zero matrix}
    The zero matrix, denoted \(0\) or \(0_{m,n}\)
    is defined as the \matrix of size \((m \times n)\)
    where every element is \(0\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-addition-definition}{Matrix addition}
    Two matrices \(A\) and \(B\) of the same size
    with elements \(a_{i,j}\) and \(b_{i,j}\) respectively
    can be added. The resulting \matrix \(A+B=C\)
    has elements \(c_{i,j} = a_{i,j} + b_{i,j}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-scalar-multiplication-definition}{Matrix scalar multiplication}
    A \matrix \(A\) with elements \(a_{i,j}\)
    can be multiplied by a scalar \(k\).
    The resulting \matrix \(kA=B\) has elements
    \(b_{i,j} = k \cdot a_{i,j}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-multiplication-definition}{Matrix multiplication}
    Two matrices \(A\) of size \((m \times n)\) and
    \(B\) of size \((p \times q)\)
    with elements \(a_{i,j}\) and \(b_{i,j}\) respectively
    can be multiplied if \(n=p\).
    The resulting \matrix \(AB=C\) has elements
    \[
        c_{i,j} = \sum_{k=1}^n
        a_{i,k}b_{k,j}
    \]
    with \(1 \leq i \leq m\)
    and \(1 \leq j \leq q\).
\end{snippetdefinition}

\begin{snippetdefinition}{kronecker-delta-definition}{Kronecker delta}
    The \emph{Kronecker delta} is a \function of two variables
    defined as follows:
    \[
        \delta_{i,j} = \begin{cases}
            0 & i \neq j \\
            1 & i = j
        \end{cases}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{identity-matrix-definition}{Identity matrix}
    The \textit{identity matrix}, denoted \(I_n\),
    is a square \matrix of side \(n\)
    with entries \(\kronecker_{i,j}\).
    \[
    I_n=
        \begin{bmatrix}
            1 & 0 & 0 & \cdots & 0 \\
            0 & 1 & 0 & \cdots & 0 \\
            0 & 0 & 1 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & 1 \\
        \end{bmatrix}
    \]
\end{snippetdefinition}

\begin{snippetproposition}{kronecker-delta-sifting-property}{Kronecker delta sifting property}
    Let \(x_1, x_2, \cdots, x_n \in \realnumbers\)
    and \(1 \leq k \leq n\).
    \[
        \sum_{i=1}^n \kronecker_{i,k}x_i = x_k
    \]
\end{snippetproposition}

\begin{snippetdefinition}{linearalgebra-matrix-premultiplication}{Matrix premultiplication}
    A \matrix \(M\) can be \textit{premultiplied}
    by another \matrix \(A\), resuling in \(AM\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-postmultiplication}{Matrix postmultiplication}
    A \matrix \(M\) can be \textit{postmultiplied}
    by another \matrix \(A\), resulting in \(MA\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-exponentiation-definition}{Matrix exponentiation}
    Given a square \matrix \(M\), the power of a matrix
    is defin as
    \[
        M^k = \underbrace{M\cdot M \cdot M \cdots M}_{k \text{ times}}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-polynomial-of-a-matrix-definition}{Polynomial of a matrix}
    Given a \polynomial \(p(x)=\sum_{n=0}^k a_nx^n\)
    and a square \matrix \(M\) we defined
    \[
        p(M) = \sum_{n=0}^k a_nM^n
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-exponentiation-definition}{Matrix Exponentiation}
    TODO
\end{snippetdefinition}

\begin{snippetexample}{linearalgebra-matrix-exponentiation-example-1}{Matrix exponentiation example 1}
    \begin{align*}
        & {\begin{bmatrix}
            \cos\alpha & \sin\alpha \\
            \sin\alpha & -\cos\alpha
        \end{bmatrix}}^2
        = \begin{bmatrix}
            \cos^2\alpha + \sin^2\alpha & \cos\alpha\sin\alpha - \sin\alpha\cos\alpha \\
            \cos\alpha\sin\alpha - \sin\alpha\cos\alpha & \cos^2\alpha + \sin^2\alpha
        \end{bmatrix}
        \\
        &= \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix} = I_2
    \end{align*}
    For all \(\alpha \in \realnumbers\)
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-exponentiation-example-2}{Matrix exponentiation example 2}
    \(\nexists A \suchthat A^2 = B\) where
    \[
        B = \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix}
    \]
    To prove this, let \(a,b,c,d \in \complexnumbers\)
    \[
        \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix}
        =
        {\begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}}^2
        =
        \begin{bmatrix}
            a^2+bc & b(a+d) \\
            c(a+d) & bc+d^2
        \end{bmatrix}
    \]
    Since \(c(a+d)=0\), then \(c=0 \land a+d=0\).
    But this contradicts \(b(a+d)=1\) meaning \(a+d \neq 0\).
\end{snippetexample}

\begin{snippetproposition}{linearalgebra-matrix-identity-postmultiplication}{Matrix Identity Postmultiplication}
    Given a \matrix \(A\),
    \[ AI_n=A \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-matrix-identity-postmultiplication-proof}{linearalgebra-matrix-identity-postmultiplication}{Matrix Identity Postmultiplication}
    Let the \matrix \(A\) be defined with the
    elements \(a_{i,j}\).
    Then, by the sifting property % TODO link
    \[
        a_{i,j} = \sum_{k=1}^n
        a_{i,j}\kronecker_{k,j} = a_{i,j}
    \]
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-matrix-identity-premultiplication}{Matrix Identity Premultiplication}
    Given a \matrix \(A\),
    \[ I_n A=A \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-matrix-identity-premultiplication-proof}{linearalgebra-matrix-identity-premultiplication}{Matrix Identity Premultiplication}
    Let the \matrix \(A\) be defined with the
    elements \(a_{i,j}\).
    Then, by the sifting property % TODO link
    \[
        a_{i,j} = \sum_{k=1}^n
        \kronecker_{i,k}a_{k,j} = a_{i,j}
    \]
\end{snippetproof}

\begin{snippetdefinition}{linearalgebra-matrix-inverse-definition}{Matrix Inverse}
    Let \(M\) be a square \matrix of side \(n\).
    A \matrix \(B\) is an \textit{inverse}
    of \(M\) if \[BM=MB=I_n\]
    The inverse of \(M\) is denoted by \(M^{-1}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-invertible-matrix-definition}{Invertible Matrix}
    A \matrix is \textit{invertible} if it has an inverse.
    Otherwise, it is \textit{singular}.
\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-uniqueness-of-matrix-inverse}{Uniqueness of \matrix inverse}
    If a \matrix \(A\) has an inverse, then it is unique.
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-uniqueness-of-matrix-inverse-proof}{linearalgebra-uniqueness-of-matrix-inverse}{Uniqueness of matrix inverse}
    Suppose a \matrix \(A\) has distinct inverses \(B\)
    and \(C\).
    It is possible to show that \(B\) and \(C\) are actually
    the same matrix.
    \[
        C=I_n C = (BA) C = B (AC) = BI_n = B
    \]
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-involution-rule-for-matrix-inverse}{Involution rule for \matrix inverse}
    If \(M\) is an invertible matrix
    \[
        {(M^{-1})}^{-1} = M
    \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-involution-rule-for-matrix-inverse-proof}{linearalgebra-involution-rule-for-matrix-inverse}{Involution rule for matrix inverse}
    Since \(M(M^{-1}) = I\), \({(M^{-1})}^{-1}\)
    by uniqueness. % TODO link
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-product-rule-of-inverse-matrices}{Product rule of inverse matrices}
    For invertible matrices \(A\) and \(B\)
    \[
        {(AB)}^{-1} = A^{-1} B^{-1}
    \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-product-rule-of-inverse-matrices-proof}{linearalgebra-product-rule-of-inverse-matrices}{Product rule of inverse matrices}
    TODO
\end{snippetproof}

\begin{snippetdefinition}{linearalgebra-matrix-left-and-write-inverses}{Matrix left and write inverses}
    Let \(A\) is an \(m \times n\) matrix. A \matrix \(B\)
    is called a \textit{right inverse} of \(A\)
    if \(AB=I_m\).
    A \matrix \(B\) is called a \textit{right inverse} of \(A\)
    if \(BA=I_n\).
\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-inverse-of-a-non-square-matrix}{Inverse of a non-square matrix}
    If \(A\) is a non-square matrix, it cannot have
    both a \textit{left inverse} and a \textit{right inverse}.
\end{snippetproposition}

\begin{snippetproposition}{linearalgebra-inverse-of-a-square-matrix}{Inverse of a square matrix}
    If \(A\) is a square matrix
    with a left inverse \(B\),
    then \(B\) is also a right inverse.
\end{snippetproposition}

\end{document}